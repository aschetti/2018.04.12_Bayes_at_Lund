---
title: Sequential Testing with Information Criteria and Evidence Ratios
subtitle: Ladislas Nalborczyk

author: Univ. Grenoble Alpes, CNRS, LPNC, 38000 Grenoble, France <br> Department of Experimental Clinical and Health Psychology, Ghent University

output:
  xaringan::moon_reader:
    #css: ["default", "bal", "bal-fonts"]
    #css: ["default", "metropolis", "metropolis-fonts"]
    css: ["default", "baltheme.css", "baltheme-fonts.css"]
    self_contained: false
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include = FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
```

# The power problem/paradox

How many participants/observations should I obtain for my next experiment, in order to show the effect I want to show ?

--

**Usual algorithm** (*a priori* NHST-PA): trying to guess the population effect size, plugging numbers (effect size and requested power) in **the magic hat** (the power function), and planning for a specific n (sample size).

```{r, echo = FALSE, dev = "svg", fig.align = "center", fig.width = 4, fig.height = 4}
knitr::include_graphics("hat.jpg")
```

---

# Sequential testing

Paradox: If I knew what (effect size) I am looking for, I would not bother making an experiment, right ?

--

Another approach consists in recruiting participants until we reach a predefined level of *evidence*. This is called **sequential testing**.

--

But this requires that we have a way to quantify evidence...

---

# Quiz: what does what ?

**Quantifying the relative evidence for an hypothesis/model**

--

&#8627; Bayes Factors

--

**Making decisions while controlling error rates in the long-run**

--

&#8627; NHST & p-values (à la Neyman-Pearson)

--

**Examining predictive abilities of models**

--

&#8627; Information Criteria (e.g., AIC, QAIC, WAIC)

--

The above methods rest on quite different philosophical/theoretical/statistical grounds, so please do not mix up your goals with the reason for which each method was designed.

---

# Sequential testing

Example of the *Sequential Bayes Factor* procedure developed by Schönbrodt et al. (2017).

```{r, echo = FALSE, dev = "svg", fig.align = "center"}
knitr::include_graphics("BFDA1.gif")
```

---

# AIC and deviance

Akaike (1971) noticed a mindblowing fact. He realised that the AIC, defined as the negative log-likehood of a model + 2 times the number of parameters was approximately equal to the **out-of-sample deviance** of a model.

--

$$\text{AIC} = -2\log(\mathcal{L}(\hat{\theta}|\text{data}))+2K$$

--

**In-sample deviance**: how bad is a model to explain the current dataset (the dataset that we used to fit the model)

--

**Out-of-sample deviance**: how bad is a model to explain a **future** dataset issued from the same data generation process (the same population)

--

This is crazy no ?

---

# AIC, Akaike weights and evidence ratios

The individual AIC values are not interpretable in absolute terms as they contain arbitrary constants and are much affected by sample size. Then it is imperative to rescale these criteria. Usually, this is done by substracting to the AIC of each model the AIC of the model with the minimum one:

$$\Delta_{AIC} = AIC_{i} - AIC_{min}$$

--

It is convenient to normalize the model likelihoods such that they sum to 1 and treat them as probabilities. Hence, we use:

$$w_{i}=\dfrac{exp(-\Delta_{i}/2)}{\sum_{r=1}^{R}exp(-\Delta_{r}/2)}$$

---

# AIC, Akaike weights and evidence ratios

The $w_i$, called **Akaike weights**, are useful as the *weight of evidence* in favor of model $g_i(\cdot |\theta)$ as being the actual Kullback-Leibler best model in the set. The ratios $w_i/w_j$ are identical to the original likelihood ratios, $\mathcal{L}(g_i|data)/\mathcal{L}(g_j|data)$, and so they are invariant to the model set, but the $w_i$ values depend on the full model set because they sum to 1.

--

Evidence can be judged by the relative likelihood of model pairs as $\mathcal{L}(g_i|x)/\mathcal{L}(g_j|x)$ or, equivalently, the ratio of Akaike weights $w_i/w_j$. Such ratios are called **evidence ratios** and represend the evidence about fitted models as to which is better in a Kullback-Leibler information sense.

---

# Sequential testing

But BFs and AIC-based ERs have different asymptotic properties (because they answer different questions).

--

<div align = "center">
<img src = "bf_er.jpeg" width = 567.351 height = 300>
</div>

--

In other words, when using ERs, the reduced model (with less parameters) cannot be more than $e^{\Delta K}$ times more credible than the augmented model...

---

# AIC-based sequential testing

When using information criteria for sequential testing, the question is not anymore when do I have enough evidence for this hypothesis over this other hypothesis (the more complex model is always better, if enough data).

--

Instead, the question is when do I have "enough data” to estimate precisely the parameters of the augmented model ? When does the more complex model becomes *enough supported* by the data ?

--

In other words, *when does the augmented (the more complex) model becomes parsimonious* ?

---

# ESTER

```{r, eval = FALSE, echo = FALSE}
if (!require("devtools") ) install.packages("devtools")
devtools::install_github("lnalborczyk/ESTER", dependencies = TRUE)
```

```{r, eval = TRUE, cache = TRUE, dev = "svg", results = "hide", fig.align = "center", fig.width = 6, fig.height = 4}
library(ESTER)

simER(cohensd = 0.7, nmin = 20, nmax = 200, boundary = 10, nsims = 100, ic = aic) %>%
    plot(log = TRUE, hist = TRUE)
```

---

# Simulations

Using simulations, we can compute the *likelihood* of observing an evidence ratio $ER$ above a certain threshold $T$, given a sample size $n$ and an effect size $d$ (a kind of p-value).

$$p(ER>T|n,d)$$

---

# Simulations

```{r, eval = FALSE}
#########################################################################
# simulating sequential evidence ratios for various effect sizes
####################################################################

ds <- seq(0, 0.9, by = 0.1)

for (i in ds) {

    sim0 <-
        {{simER(
            cohensd = i, nmin = 20, nmax = 1e3, boundary = Inf,
            nsims = 1e4, ic = aic, cores = 4
            )}}

    if (!exists("sim") ) sim <- sim0 else sim <- rbind(sim, sim0)

}
```

---

# Likelihood

```{r, eval = TRUE, cache = TRUE, dev = "svg", fig.align = "center", fig.width = 6, fig.height = 4}
load(url("https://github.com/lnalborczyk/shiny/blob/master/sims.RData?raw=true") )

bayes(sim, type = "likelihood", threshold = ER > 10)
```

---

# Using Bayes

Then, using Bayes theorem, we can compute the *posterior probability* of the population effect size being $d_{i}$ (for $d$ in $d_{i}, \dots , d_{k}$), given that the evidence ratio $ER$ is above a threshold $T$, and for a sample size $n$.

--

$$p(d_{i}|ER>T,n) = \frac{p(ER>T|n,d_{i}) \cdot p(d_{i}|n)}{\sum_{i=1}^{k}{p(ER>T|n,d_{i}) \cdot p(d_{i}|n)}}$$

--

But we need to define the *prior probability* of each effect size...

---

# Defining a prior on effect sizes

Based on previous work trying to estimate the distribution of effect sizes in Psychology, we used a discrete Gaussian distribution with mean $\mu = 0.5$ and standard deviation $\sigma = 0.25$ as a prior distribution.

```{r, eval = TRUE, echo = FALSE, cache = TRUE, dev = "svg", fig.align = "center", fig.width = 7, fig.height = 4}
######################################################
# constructing a discrete normal prior
###############################################

mu <- 0.5
sigma <- 0.25

p <- c(
    dnorm(0, mu, sigma), dnorm(0.1, mu, sigma), dnorm(0.2, mu, sigma),
    dnorm(0.3, mu, sigma), dnorm(0.4, mu, sigma), dnorm(0.5, mu, sigma),
    dnorm(0.6, mu, sigma), dnorm(0.7, mu, sigma), dnorm(0.8, mu, sigma),
    dnorm(0.9, mu, sigma)
    )

# normalising it

p <- p / sum(p)

# plotting it

data.frame(x = seq(0, 0.9, 0.1), y = p) %>%
    ggplot(aes(x = x, y = y) ) +
    geom_bar(stat = "identity") +
    theme_bw(base_size = 12) +
    scale_x_continuous(breaks = seq(0, 0.9, 0.1) ) +
    xlab(expression(delta) ) +
    ylab("")
```

---

# Posterior

We can then plot the posterior distribution as a function of the sample size and for this specific prior distribution on effect sizes.

```{r, eval = TRUE, cache = TRUE, dev = "svg", fig.align = "center", fig.width = 6, fig.height = 4}
bayes(sim, type = "posterior", threshold = ER > 10, prior = p)
```

---

# Likelihood & Posterior

We can also compute the probability of observing an ER > 10 if the population effect size is equal to 0 (for instance) and n = 113, given a prior distribution on effect sizes.

```{r, eval = TRUE, cache = TRUE}
bayes(sim, type = "table", threshold = ER > 10, prior = p) %>%
    filter(n == 113 & true.ES == 0)
```

---

# Likelihood & Posterior

Or we can plot the posterior distribution of effect sizes at n = 113.

```{r, eval = TRUE, echo = FALSE, cache = TRUE, dev = "svg", fig.align = "center", fig.width = 8, fig.height = 4}
bayes(sim, type = "table", threshold = ER > 10, prior = p) %>%
    filter(n == 113) %>%
    gather(type, prob, likelihood:posterior) %>%
    filter(type %in% c("prior", "posterior") ) %>%
    ggplot(aes(x = true.ES, y = prob, fill = type) ) +
    geom_bar(stat = "identity", position = position_dodge() ) +
    theme_bw(base_size = 12) +
    scale_fill_grey(start = 0.25, end = 0.75) +
    xlab(expression(delta) ) +
    ylab("")
```

---

# Thanks

...
