<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Analyzing an experiment on   involuntary attention using brms</title>
    <meta charset="utf-8" />
    <meta name="author" content="Antonio Schettino    Department of Experimental-Clinical &amp; Health Psychology  Ghent University" />
    <link rel="stylesheet" href="Schettino_UGent_CSS.css" type="text/css" />
    <link rel="stylesheet" href="Schettino_UGent-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Analyzing an experiment on <br /> involuntary attention using <strong><code>brms</code></strong>
### Antonio Schettino<br /> <br /> Department of Experimental-Clinical &amp; Health Psychology<br /> Ghent University

---




layout: true

&lt;!-- set header --&gt;
&lt;div class="my-header"&gt;&lt;/div&gt;

&lt;!-- set footer --&gt;
&lt;div class="my-footer"&gt;&lt;span&gt;Antonio Schettino   
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
Bayes@Lund - 12.04.2018&lt;/span&gt;&lt;/div&gt;

---
name: outline

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.left[
  .font200[
    **OUTLINE**
  ]
]

&lt;br /&gt;

.font120[
  * background
]

.font120[
  * experimental paradigm
]

.font120[
  * data and results
]

.font120[
  * conclusions
]

&lt;!-- notes --&gt;

???

Good day everyone, my name is Antonio Schettino and I am a post-doctoral researcher at Ghent University.   
Today I will show you the analyses (in progress) of some data collected during an experiment on involuntary spatial attention. These analyses are conducted in *R* using the package `brms` by Paul Buerkner, one of the keynote speakers of this meeting.   

* I will first give a broad background on what we are investigating.
* Then I will show the experimental paradigm we used to address the issue.
* I will then show you the data and mention what kind of analysis we performed.
* Finally, I will give an overview of what we were able to learn with these analyses.

&lt;!-- end notes --&gt;

--
&lt;Div style="margin-top:-380px" /&gt;

.right[
  ![collaborators](media/collaborators.jpg)
]

&lt;!-- notes --&gt;

???

Before starting, I would like to acknowledge the contribution of these people:

* Sarina Evens collected the data of the pilot study and crafted a remarkable master thesis,
which you can find on the [Open Science Framework](https://osf.io/azh5r/)
* Annelies Elegeert collected the data of the experiment I will present today
* Valentina Rossi and Gilles Pourtois are long-time collaborators who helped with the experimental design and framing the theoretical background

&lt;!-- end notes --&gt;

---
name: notification_trolling

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Involuntary attention**
  ]
]

&lt;br /&gt;

.center[
  &lt;body&gt;
  &lt;video width="320" height="240" controls="controls" vid.autoplay="false"&gt;
  &lt;source src="media/notification_troll.mp4" type="video/mp4"&gt;
  &lt;object data="" width="320" height="240"&gt;
  &lt;embed width="320" height="240" src="notification_troll.mp4"&gt;
  &lt;/object&gt;&lt;/video&gt;&lt;/body&gt;
  &lt;br /&gt;&lt;br /&gt;
  source: https://www.youtube.com/watch?v=0M2L9XNYfLs
]

&lt;!-- notes --&gt;

???

Let's start by clarifying what we wanted to study: involuntary attention.   
   
Instead of giving a definition, perhaps a video might show this process more intuitively.

&lt;!-- end notes --&gt;

---
name: clarify_example1

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**What happened?**
  ]
]

&lt;!-- notes --&gt;

???

So, what happened here?

&lt;!-- end notes --&gt;

--

.font120[
  * The guy in the background was attracted to the phone...
]

&lt;!-- notes --&gt;

???

The guy in the background was attracted to the phone *despite knowing* that his friend was making the notification sounds. In other words, his attention was **automatically** attracted to his phone.

&lt;!-- end notes --&gt;

--

.font120[
  * ... despite *knowing* that his friend was making the notification sounds
]

--

.font120[
  * His attention was **automatically** attracted to ~~uninformative~~ **counterproductive** sounds (distraction from current task: watching TV)
]

&lt;!-- notes --&gt;

???

Note that these sounds were not just uninformative (they were not signalling a real message), but were **counterproductive**, because they distracted him from his current task (watching TV).

&lt;!-- end notes --&gt;

---
name: clarify_example2

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Why study involuntary attention?**
  ]
]

.font120[
  * Involuntary attentional orienting can be dangerous in some real-life situations
]


&lt;!-- notes --&gt;

???

This is just a funny example, but involuntary attention can also lead to dangerous consequences in real life. 

&lt;!-- end notes --&gt;

--

.font120[
  *  Car driver distracted by flashing mobile phone, worker operating heavy machinery distracted by blinking lights, ...
]


&lt;!-- notes --&gt;

???
 
For example, imagine driving your car in a busy road and suddenly being distracted by a flash on your mobile phone.
You could cause an accident.   
Imagine a worker operating a hydraulic press and being distracted by a blinking light. They could get seriously injured.

&lt;!-- end notes --&gt;

--

.font120[
  * How to study this phenomenon in the lab?
]

&lt;!-- notes --&gt;

???

We need to understand this phenomenon deeply, but first we need to do it in controlled situations. So how can we study it in the lab?

&lt;!-- end notes --&gt;

---
name: TOJ

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Temporal Order Judgment (TOJ)**
  ]
]

&lt;!-- notes --&gt;

???

One way would be to use a **temporal order judgment task**.

&lt;!-- end notes --&gt;


--

.font120[
  * Flash two stimuli on screen
]

--

.font120[
  * Task: which stimulus appeared **first**?
]

&lt;!-- notes --&gt;

???

In the simplest version of this experimental paradigm, two stimuli are briefly flashed on the screen and
observers have to say which stimulus appeared first.

&lt;!-- end notes --&gt;

--

.font120[
  * Difficulty depends on the time between the onset of the stimuli (**SOA**)
]

&lt;!-- notes --&gt;

???

Difficulty varies as a function of Stimulus Onset Asynchrony (SOA), which is the time between the onset of the stimuli.

&lt;!-- end notes --&gt;

---
name: TOJ_example
background-image: url("media/TOJ_video.gif")
background-size: 700px
background-position: 50% 65%

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Temporal Order Judgment (TOJ)**
  ]
]

&lt;!-- notes --&gt;

???

This is an example of an easy trial.   
Here the first stimulus (vertical lines) appear on the right side, followed by the second stimulus (horizontal lines). Participants have to judge which lines appear first.   
Changing the time between the onset of the two stimuli makes it easier or more difficult.   
Here timing is not correct due to technical limitations of the presentation, but at least you get my point.

&lt;!-- end notes --&gt;

---
name: TOJ_cue

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Counterproductive TOJ**
  ]
]

.font120[
  * An **exogenous cue** is used to attract attention towards one placeholder
]

&lt;!-- notes --&gt;

???

In our modified version of this paradigm, we use an exogenous cue to attract attention to one of the two placeholders.

&lt;!-- end notes --&gt;

--

.font120[
  * The stimulus on the attended location is perceived as **first** even when appearing second
]

&lt;!-- notes --&gt;

???

This creates the illusion of perceiving the stimulus at the attended location as appearing first **even when it's not true**.

&lt;!-- end notes --&gt;

--

.font120[
  * What if the cue is **always wrong**, i.e., appearing on the location of the *second* stimulus?
]

&lt;!-- notes --&gt;

???

Now, what would happen if we present a cue that is **always wrong**, always appearing on the location of the second stimulus? Would participants' attention still automatically shift towards that location, even though it's counterproductive? This would be strong evidence that people cannot suppress this automatic urge to look at the cued location.

&lt;!-- end notes --&gt;

---
name: TOJ_video_cue
background-image: url("media/TOJ_video_cue.gif")
background-size: 700px
background-position: 50% 80%

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Counterproductive TOJ**
  ]
]

&lt;!-- notes --&gt;

???

This is an example of this modified paradigm.   
Here the cue (a thick placeholder box) appears on the left, then the first stimulus (horizontal lines) appear on the right side, followed by the second stimulus (the vertical lines).   
So, the correct answer would be to judge the horizontal lines as appearing first but, because of the cue, participants should be more inclined to say that the **vertical lines** appeared first.   
Again, timing here is not correct, this example is just to get the point across.

&lt;!-- end notes --&gt;

---
name: TOJ_graph

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**TOJ - Data**
  ]
]

&lt;Div style="margin-top:-40px" /&gt;

.center[

![](Schettino_BayesAtLund2018_files/figure-html/TOJ_plot-1.svg)&lt;!-- --&gt;

]

&lt;!-- notes --&gt;

???

This is a visualization of the data.   
   
On the x-axis you see the SOAs. Positive SOAs mean that the horizontal lines were presented first, negative SOAs mean that the vertical lines were presented first.   
   
On the y-axis you see the proportion of times people judged the horizontal lines as appearing first.   
X-axis: FACT. Y-axis: PERCEPTION.   
   
The black line represents responses when no cue was presented, our baseline condition.   
When the vertical lines appear long before the horizontal lines, people should almost never judge the horizontal lines as appearing first. This is what we see here.   
The shorter the SOA, the more uncertain people are in their judgment. When the stimuli are presented simultaneously,
there is a 50/50 chance of perceiving the horizontal or vertical lines as first.   
   
What happens when the horizontal lines are cued (blue line)?   
If the horizontal lines are cued, it means that the vertical lines always appeared first. However, participants say more often that the horizontal lines appeared first because they were cued.   
The reverse effect can be seen when the vertical lines were cued.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_desc1

&lt;!-- Incremental lists do not give me what I want (due to increased font formatting and spacing), --&gt;
&lt;!-- so I created new slides and duplicated the content. Not elegant, but effective. --&gt;

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Bayesian multilevel modeling**
  ]
]

&lt;!-- notes --&gt;

???

We analyzed these data using Bayesian multilevel modeling in `brms`.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_desc2

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Bayesian multilevel modeling**
  ]
]

.font120[
* logistic regression, varying intercepts &amp; slopes on *participants*
]


&lt;!-- notes --&gt;

???
 
Because the data were binary (a bunch of 2s and 8s), we fitted multilevel logistic regressions and allowed participants' intercepts and slopes to vary.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_desc3

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Bayesian multilevel modeling**
  ]
]

.font120[
* logistic regression, varying intercepts &amp; slopes on *participants*
* **highly informative priors** (from a pilot study)
]

&lt;!-- notes --&gt;

???

On all the parameters of interest, we placed highly informative priors (posterior distributions obtained in a pilot study).

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_desc4

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Bayesian multilevel modeling**
  ]
]

.font120[
* logistic regression, varying intercepts &amp; slopes on *participants*
* **highly informative priors** (from a pilot study)
* model comparison:
  1. full model (**SOA** + **cue** + **SOA** x **cue**)
  2. main effects (**SOA** + **cue**)
  3. main effect of **SOA**
  4. main effect of **cue**
  5. **null** model (intercept only)
]

&lt;!-- notes --&gt;

???

We fitted several models and quantified which one had the best predictive validity.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_desc5

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Bayesian multilevel modeling**
  ]
]

.font120[
* logistic regression, varying intercepts &amp; slopes on *participants*
* **highly informative priors** (from a pilot study)
* model comparison:
  1. full model (**SOA** + **cue** + **SOA** x **cue**)
  2. main effects (**SOA** + **cue**)
  3. main effect of **SOA**
  4. main effect of **cue**
  5. **null** model (intercept only)
* on the winning model:
  - diagnostics &amp; posterior predictive checks
  - hypothesis testing
]

&lt;!-- notes --&gt;

???

Once we identified the winning model, we ran diagnostics and posterior predictive checks and did some hypothesis testing.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_model

&lt;!-- Incremental lists do not give me what I want (due to increased font formatting and spacing), --&gt;
&lt;!-- so I created new slides and duplicated the content. Not elegant, but effective.              --&gt;

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Bayesian multilevel modeling with ```brms```**
  ]
]

&lt;Div style="margin-top:-30px" /&gt;

```r
##################### full model (SOA * cue) #####################
model.full &lt;- brm(num.horiz1st | trials(tot.trials) ~ SOA * cue +
              (SOA * cue || participant), 
              data = data.TOJ,
              family = binomial("logit"),
              prior = priors.full,
              sample_prior = TRUE,
              inits = "random",
              control = list(adapt_delta = .9),
              chains = 4,
              iter = 2000,
              warmup = 500,
              thin = 1,
              algorithm = "sampling",
              cores = 4,
              seed = 9001)
```


&lt;!-- notes --&gt;

???

This is the syntax in `brms`.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_model_highlight1

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Bayesian multilevel modeling with ```brms```**
  ]
]

&lt;Div style="margin-top:-30px" /&gt;

```r
##################### full model (SOA * cue) #####################
*   model.full &lt;- brm(num.horiz1st | trials(tot.trials) ~ SOA * cue +
              (SOA * cue || participant),
              data = data.TOJ,
              family = binomial("logit"),
              prior = priors.full,
              sample_prior = TRUE,
              inits = "random",
              control = list(adapt_delta = .9),
              chains = 4,
              iter = 2000,
              warmup = 500,
              thin = 1,
              algorithm = "sampling",
              cores = 4,
              seed = 9001)
```

&lt;!-- notes --&gt;

???
   
First, you should specify the model formula. Here, the dependent variable is the number of trials in which participants responded "horizontal first" over all trials.   
   
*Conditions* is the name of the variable containing all condition combinations (e.g., no cue at *SOA-217*, horizontal cued at *SOA+83*, ...). It is our **population-level** (or **fixed**) effect.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_model_highlight2

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Bayesian multilevel modeling with ```brms```**
  ]
]

&lt;Div style="margin-top:-30px" /&gt;

```r
##################### full model (SOA * cue) #####################
model.full &lt;- brm(num.horiz1st | trials(tot.trials) ~ SOA * cue +
*                (SOA * cue || participant),
              data = data.TOJ,
              family = binomial("logit"),
              prior = priors.full,
              sample_prior = TRUE,
              inits = "random",
              control = list(adapt_delta = .9),
              chains = 4,
              iter = 2000,
              warmup = 500,
              thin = 1,
              algorithm = "sampling",
              cores = 4,
              seed = 9001)
```

&lt;!-- notes --&gt;

???

Then we specify the group-level (or **random**) effects. In this case, we allow intercepts and slopes to vary across participants for each condition combination.   
What does it practically mean? For example:

* participant 1 could overall be slower than participant 2; or
* participant 17 could be faster at SOA 217 but slower at SOA150 compared to participant 19;
* ... and so on.

Here, `||` is used to prevent group-level correlations from being modeled.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_model_highlight3

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Bayesian multilevel modeling with ```brms```**
  ]
]

&lt;Div style="margin-top:-30px" /&gt;

```r
##################### full model (SOA * cue) #####################
model.full &lt;- brm(num.horiz1st | trials(tot.trials) ~ SOA * cue +
              (SOA * cue || participant),
*                data = data.TOJ,
              family = binomial("logit"),
              prior = priors.full,
              sample_prior = TRUE,
              inits = "random",
              control = list(adapt_delta = .9),
              chains = 4,
              iter = 2000,
              warmup = 500,
              thin = 1,
              algorithm = "sampling",
              cores = 4,
              seed = 9001)
```

&lt;!-- notes --&gt;

???

`data.TOJ` is the name of the data frame containing your data.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_model_highlight4

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Bayesian multilevel modeling with ```brms```**
  ]
]

&lt;Div style="margin-top:-30px" /&gt;

```r
##################### full model (SOA * cue) #####################
model.full &lt;- brm(num.horiz1st | trials(tot.trials) ~ SOA * cue +
              (SOA * cue || participant),
              data = data.TOJ,
*                family = binomial("logit"),
              prior = priors.full,
              sample_prior = TRUE,
              inits = "random",
              control = list(adapt_delta = .9),
              chains = 4,
              iter = 2000,
              warmup = 500,
              thin = 1,
              algorithm = "sampling",
              cores = 4,
              seed = 9001)
```

&lt;!-- notes --&gt;

???

Then we specify the likelihood function. The data are a bunch of 2s and 8s (the participant did/didn't respond "horizontal first"), hence the binomial distribution. The *logit* link allows us to interpret the resulting coefficients in terms of odds ratios.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_model_highlight5

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Bayesian multilevel modeling with ```brms```**
  ]
]

&lt;Div style="margin-top:-30px" /&gt;

```r
##################### full model (SOA * cue) #####################
model.full &lt;- brm(num.horiz1st | trials(tot.trials) ~ SOA * cue +
              (SOA * cue || participant),
              data = data.TOJ,
              family = binomial("logit"),
*                prior = priors.full,
              sample_prior = TRUE,
              inits = "random",
              control = list(adapt_delta = .9),
              chains = 4,
              iter = 2000,
              warmup = 500,
              thin = 1,
              algorithm = "sampling",
              cores = 4,
              seed = 9001)
```

&lt;!-- notes --&gt;

???
   
`priors.full` indicates a list with highly informative priors. We ran a pilot study identical to this one, fitted this same model (with default priors), and used the posterior distributions of those parameters as priors for this model.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_model_highlight6

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Bayesian multilevel modeling with ```brms```**
  ]
]

&lt;Div style="margin-top:-30px" /&gt;

```r
##################### full model (SOA * cue) #####################
model.full &lt;- brm(num.horiz1st | trials(tot.trials) ~ SOA * cue +
              (SOA * cue || participant),
              data = data.TOJ,
              family = binomial("logit"),
              prior = priors.full,
*                sample_prior = TRUE,
              inits = "random",
              control = list(adapt_delta = .9),
              chains = 4,
              iter = 2000,
              warmup = 500,
              thin = 1,
              algorithm = "sampling",
              cores = 4,
              seed = 9001)
```

&lt;!-- notes --&gt;

???

We tell `brms` to extract samples from the priors... they will be useful later on.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_model_highlight7

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Bayesian multilevel modeling with ```brms```**
  ]
]

&lt;Div style="margin-top:-30px" /&gt;

```r
##################### full model (SOA * cue) #####################
model.full &lt;- brm(num.horiz1st | trials(tot.trials) ~ SOA * cue +
              (SOA * cue || participant),
              data = data.TOJ,
              family = binomial("logit"),
              prior = priors.full,
              sample_prior = TRUE,
*                inits = "random",
*                control = list(adapt_delta = .9),
*                chains = 4,
*                iter = 2000,
*                warmup = 500,
*                thin = 1,
*                algorithm = "sampling",
*                cores = 4,
*                seed = 9001)
```

&lt;!-- notes --&gt;

???

The following settings refer to the No-U-Turn-Sampler (*NUTS*) in STAN:

* `inits`: initial parameter values in the Markov-Chain Monte Carlo (MCMC) chains;
* `control`: steps of the NUTS sampler;
* `chains`: number of MCMC chains;
* `iter`: number of iterations for each MCMC chain;
* `warmup`: number of burn-in samples (not included in final posterior distribution);
* `thin`: thinning rate (to decrease autocorrelation);
* `algorithm`: type of sampling algorithm;
* `cores`: numer of processor cores for parallel computations;
* `seed`: seed for RNG (to ensure reproducible results).

For details, see `help(brm)`.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_model_highlight8

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Bayesian multilevel modeling with ```brms```**
  ]
]

&lt;Div style="margin-top:-30px" /&gt;

```r
##################### main effect of SOA #####################
*   model.SOA &lt;- brm(num.horiz1st | trials(tot.trials) ~ SOA +
*                 (SOA || participant),
              data = data.TOJ,
              family = binomial("logit"),
*                 prior = priors.SOA,
              sample_prior = TRUE,
              inits = "random",
              control = list(adapt_delta = .9),
              chains = 4,
              iter = 2000,
              warmup = 500,
              thin = 1,
              algorithm = "sampling",
              cores = 4,
              seed = 9001)
```

&lt;!-- notes --&gt;

???

To specify the other models, change the name of the independent variable and set the priors of the appropriate model.

&lt;!-- end notes --&gt;

---
name: TOJ_LOO

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Model comparison with ```brms```**
  ]
]

&lt;Div style="margin-top:-30px" /&gt;

.center[
  .font130[(*leave-one-out* cross-validation)
  ]
]

```r
model.comparison &lt;- LOO( # list with all models
                      model.full, model.mains, model.SOA, model.cue, model.null,
                      reloo = TRUE, # exact CV for problematic observations
                      compare = FALSE) # do not compare models with each other
```

.center[
  .font130[
  
  ```
  ##   models  LOO.IC
  ## 1  mains 2727.31
  ## 2   full 2844.49
  ## 3    SOA 3628.22
  ## 4    cue 7998.10
  ## 5   null 8275.49
  ```
  ]
]

&lt;!-- notes --&gt;

???

We fitted all the models and compared their out-of-sample predictive validity through leave-one-out cross-validation. We iteratively fit each model on all observations except one, and then try to predict the one we left out.   
   
The results indicated that the model with the lowest information loss was the one with both main effects, but no interaction. In other words, the effects of SOA and cue **independently** affect responses but do not interact with each other.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_output

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**```brms``` output**
  ]
]

&lt;Div style="margin-top:-40px" /&gt;

.center[
  .font130[(**main effects** model, only **constant** effects)
  ]
]


```
##  [1] "                   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS"
##  [2] "Intercept              0.06      0.06    -0.06     0.19 1.00     4764     4896"
##  [3] "SOAM217               -2.64      0.13    -2.89    -2.38 1.00     4312     4752"
##  [4] "SOAM150               -2.23      0.13    -2.50    -1.97 1.00     3746     4311"
##  [5] "SOAM83                -1.53      0.10    -1.73    -1.34 1.00     4977     4815"
##  [6] "SOAM17                -0.35      0.06    -0.47    -0.22 1.00     8996     5196"
##  [7] "SOAP17                 0.21      0.06     0.09     0.33 1.00    10682     4810"
##  [8] "SOAP83                 1.60      0.11     1.39     1.81 1.00     4316     4543"
##  [9] "SOAP150                2.42      0.16     2.11     2.72 1.00     3433     4078"
## [10] "SOAP217                2.73      0.16     2.41     3.06 1.00     3884     4483"
## [11] "cuevertical.cued      -0.80      0.06    -0.93    -0.68 1.00     6545     4869"
## [12] "cuehorizontal.cued     0.86      0.07     0.73     1.00 1.00     5432     5038"
## [13] ""
```

&lt;!-- notes --&gt;

???

This is the output that `brms` gives you. For brevity, here I am only showing the output of the constant effects.   
   
You can see mean, estimated error, and 95% credible intervals of the posterior distributions of all the parameters of this model (the intercept is SOA0, no cue).   
   
"Effective sample" is an estimate of the number of independent draws from the posterior distribution. The closer this number is to the total number of samples (in this case, 6,000), the better the estimation. As a rule of thumb, the ratio between the effective and total samples should not be lower than **0.1**.   
   
"Rhat" measures the ratio of the average variance of samples within each chain to the variance of the pooled samples between chains. If all chains converge (i.e., they are at equilibrium), Rhat = 1. As a rule of thumb, Rhat should not be higher than **1.05**.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_MCMCchains

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**MCMC chains**
  ]
]

&lt;br /&gt;
&lt;Div style="margin-left:-50px" /&gt;

.pull-left[
  ```r
  library(bayesplot)
  mcmc_trace(as.array(model.mains),
  pars = c("b_Intercept", 
    "b_cuevertical.cued", 
    "b_cuehorizontal.cued"),
  facet_args = list(ncol=1))
  ```
]

&lt;Div style="margin-right:-40px" /&gt;
&lt;Div style="margin-top:-100px" /&gt; 

.pull-right[

![](Schettino_BayesAtLund2018_files/figure-html/model_mains_MCMC-1.svg)&lt;!-- --&gt;

]

&lt;!-- notes --&gt;

???

You can visually explore the MCMC chains, to see whether they converged. For these representative parameters, the chains converged nicely onto the same parameter space ("fat hairy caterpillar").

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_PPC

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Posterior Predictive Checks**
  ]
]

&lt;br /&gt;
&lt;Div style="margin-left:-50px" /&gt;

.code100[
  .pull-left[
    ```r
    pp_check(model.mains, 
    nsamples = NULL, 
    type = "stat_grouped", 
    group = "cue")
    ```
  ]
]

&lt;Div style="margin-right:-40px" /&gt;
&lt;Div style="margin-top:-100px" /&gt; 

.pull-right[

![](Schettino_BayesAtLund2018_files/figure-html/model_mains_PPC-1.svg)&lt;!-- --&gt;

]

&lt;!-- notes --&gt;

???

One way of verifying whether your model can adequately predict unobserved data is by running *graphical posterior predictive checks*. Here the blue line is the mean of the observed data and the yellow histograms represent the posterior distributions of these parameters.   

We can see that observed and estimated data are quite similar.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_predicted_observed

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Observed vs. predicted data**
  ]
]

&lt;Div style="margin-top:-40px" /&gt;

.center[

![](Schettino_BayesAtLund2018_files/figure-html/model_mains_obs_pred-1.svg)&lt;!-- --&gt;
]

&lt;!-- notes --&gt;

???

This is another way of showing that the model generates data very similar to the observed data.   
This is the same graph that I showed you earlier, but I also added the data generated by the model (dotted lines). Observed and predicted data are almost undistinguishable, suggesting that the same process may have generated them.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_hyptest_nocue_vertcued

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Hypothesis testing**
  ]
]

&lt;Div style="margin-top:-40px" /&gt;

.center[
  .font130[(**no cue** vs. **vertical cued** conditions)
  ]
]



&lt;Div style="margin-left:-40px" /&gt;

.code100[
  .pull-left[
  ```r
  # posterior probability
  # under the hypothesis
  # (no.cue=vertical.cued)
  # against its alternative
  # (no.cue=/=vertical.cued)
  hypothesis(model.mains, 
  "Intercept = Intercept + cuevertical.cued")
  ```
  ]
]

&lt;br /&gt;
&lt;Div style="margin-right:-40px" /&gt;
&lt;Div style="margin-top:-100px" /&gt; 

.pull-right[

![](Schettino_BayesAtLund2018_files/figure-html/model_mains_hyptest_nocue_vertcued_graph-1.svg)&lt;!-- --&gt;
]

&lt;!-- notes --&gt;

???

Now, suppose we are interested in testing whether responses in the no cue condition are different from the vertical cued condition. Through the `hypothesis` function in `brms` we can test whether the 95% credible interval of the difference between these two posterior probabilities includes 0.   
   
As you can see here, the two distributions are clearly different (there is no overlap), so we can confidently conclude that "horizontal first" responses are much more likely in the no cue condition compared to the vertical cued condition.

&lt;!-- end notes --&gt;

---
name: TOJ_analysis_hyptest_nocue_prior_posterior

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Hypothesis testing**
  ]
]

&lt;Div style="margin-top:-40px" /&gt;

.center[
  .font130[(**pilot** vs. **current** experiment)
  ]
]

![](Schettino_BayesAtLund2018_files/figure-html/model_mains_hyptest_prior_vs_post_graph-1.svg)&lt;!-- --&gt;

&lt;!-- notes --&gt;

???

In the same way, we can test whether the results of this experiment are different from the results of the pilot experiment by simply comparing the prior and posterior distributions of our parameters of interest.   
   
In this case, we can see a great overlap, indicating no difference between pilot and current experiment.

&lt;!-- end notes --&gt;

---
name: conclusions1

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Conclusions**
  ]
]

.font110[
  What we have learned:
]

&lt;!-- notes --&gt;

???

So, what have learned?

&lt;!-- end notes --&gt;

---
name: conclusions2

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Conclusions**
  ]
]

.font110[
  What we have learned:
  * SOA and cue influence performance **independently**
    - comparison of *theoretically plausible* multilevel models
]

&lt;!-- notes --&gt;

???

We now know that SOA and cue affect performance independently. We reached this conclusion by comparing the predictive validity of all theoretically plausible models via leave-one-out cross-validation procedures.

&lt;!-- end notes --&gt;

---
name: conclusions3

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Conclusions**
  ]
]

.font110[
  What we have learned:
  * SOA and cue influence performance **independently**
    - comparison of *theoretically plausible* multilevel models
  * the winning model is the best in terms of **predictive accuracy**
]

---
name: conclusions4

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Conclusions**
  ]
]

.font110[
  What we have learned:
  * SOA and cue influence performance **independently**
    - comparison of *theoretically plausible* multilevel models
  * the winning model is the best in terms of **predictive accuracy**
  * **observed** and **predicted** data are very similar
]

&lt;!-- notes --&gt;

???

We have visually checked that the winning model can predict unobserved data pretty well.

&lt;!-- end notes --&gt;

---
name: conclusions5

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Conclusions**
  ]
]

.font110[
  What we have learned:
  * SOA and cue influence performance **independently**
    - comparison of *theoretically plausible* multilevel models
  * the winning model is the best in terms of **predictive accuracy**
  * **observed** and **predicted** data are very similar
  * *"horizontal first"* responses are less likely when the *vertical* lines are cued
    - comparison of the posterior distributions of no cue and vertical cued conditions
]

&lt;!-- notes --&gt;

???

In line with our predictions, we have verified that "horizontal first" responses are less likely when the vertical lines are cued compared to when no cue is presented. This was possible by comparing the posterior distributions of these two parameters and show that their 95% credible intervals do not overlap.

&lt;!-- end notes --&gt;

---
name: conclusions6

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Conclusions**
  ]
]

.font110[
  What we have learned:
  * SOA and cue influence performance **independently**
    - comparison of *theoretically plausible* multilevel models
  * the winning model is the best in terms of **predictive accuracy**
  * **observed** and **predicted** data are very similar
  * *"horizontal first"* responses are less likely when the *vertical* lines are cued
    - comparison of the posterior distributions of no cue and vertical cued conditions
  * data of **pilot** and **current** experiment are very similar
    - comparison of prior and posterior distributions of cue conditions
]

&lt;!-- notes --&gt;

???

In the same way, we were also able to show (by comparing prior and posterior distributions) that "horizontal first" response during the pilot and current experiment are virtually identical.

&lt;!-- end notes --&gt;

---
name: conclusions7

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font200[**Conclusions**
  ]
]

.font110[
  What we have learned:
  * SOA and cue influence performance **independently**
    - comparison of *theoretically plausible* multilevel models
  * the winning model is the best in terms of **predictive accuracy**
  * **observed** and **predicted** data are very similar
  * *"horizontal first"* responses are less likely when the *vertical* lines are cued
    - comparison of the posterior distributions of no cue and vertical cued conditions
  * data of **pilot** and **current** experiment are very similar
    - comparison of prior and posterior distributions of cue conditions
  * ... and much more! Thanks for ```brms```, @paulbuerkner!
]

&lt;!-- notes --&gt;

???

There is so much more that can be done in `brms`... thank you Paul!

&lt;!-- end notes --&gt;

---
name: thanks

&lt;!-- set FontAwesome icons --&gt;
&lt;link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"&gt;

&lt;Div style="margin-top:90px" /&gt; &lt;!-- start below university header --&gt;

.center[
  .font300[**Thanks for your attention!**]
]

&lt;br /&gt;

.center[
  .font150[
    &lt;a href="mailto:antonio.schettino@ugent.be"&gt; &lt;i class="fa fa-paper-plane fa-fw" style="font-size:30px;color:#152bda;"&gt; &lt;/i&gt;&amp;nbsp; antonio.schettino@ugent.be&lt;/a&gt;&lt;br&gt;
    &lt;a href="https://asch3tti.netlify.com/"&gt;&lt;i class="fa fa-link fa-fw" style="font-size:30px;color:black;"&gt;&lt;/i&gt;&amp;nbsp; asch3tti.netlify.com&lt;/a&gt;&lt;br&gt;
    &lt;a href="https://twitter.com/asch3tti"&gt;&lt;i class="fa fa-twitter fa-fw" style="font-size:35px;color:#00aced;"&gt;&lt;/i&gt;&amp;nbsp; @asch3tti&lt;/a&gt;&lt;br&gt;
&lt;br /&gt;
Slides available here: 
&lt;br /&gt;
https://asch3tti.netlify.com/post/bayesatlund2018/
  ]
]

&lt;!-- notes --&gt;

???

I would also like to thank you for your attention.   
   
If you want to keep in touch, this is my email, website, and Twitter handle.   
   
You can also find these slides on this blog post on my website.   

&lt;!-- end notes --&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="libs/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"self_contained": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
